\documentclass[11pt]{report}

% ------------------------------------------------------------------------------
% Dependencies

%\usepackage{cmbright}             % Computer Modern Bright
%\usepackage{eucal}                % Euler (calligraphic/script)
\usepackage{bm}                    % Bold math font
\usepackage[at]{easylist}          % Easy-to-use lists
\usepackage{enumitem}              % Customize `enumerate' lists
\usepackage{amsmath}               % The AMS math package
\usepackage{amssymb}               % The AMS symbols package
\usepackage{mathtools}             % Extensions to `amsmath'
\usepackage{wasysym}               % Various symbols, including smiley faces
\usepackage{stmaryrd}              % Logic and CS symbols
\usepackage{xfrac}                 % Split level fractions with \sfrac
\usepackage{bussproofs}            % Gentzen-style inference rules
\usepackage{fontspec}              % Fonts
\usepackage{listings}              % Code listings
\usepackage{color}                 % Color
\usepackage{xcolor}                % Color
\usepackage[binary-units]{siunitx} % SI units
\usepackage{parskip}

\usepackage[chapter]{tocbibind}

% ------------------------------------------------------------------------------
% Watermarks

\usepackage{draftwatermark}
\SetWatermarkText{Draft}
\SetWatermarkScale{2}
\SetWatermarkColor[gray]{0.95}

% ------------------------------------------------------------------------------
% Chapter titles

\usepackage{titlesec}
\definecolor{gray75}{gray}{0.75}
\titleformat{\chapter}[hang]{\Huge\bfseries}{%
  \thechapter\hspace{20pt}\textcolor{gray75}{|}\hspace{20pt}%
}{0pt}{\Huge\bfseries}
\titlespacing*{\chapter}{0pt}{-20pt}{40pt}

% ------------------------------------------------------------------------------
% Code blocks

\usepackage[outputdir=../out,chapter]{minted}

\usemintedstyle[haskell]{trac}

\setmonofont{DejaVu Sans Mono}[Scale=MatchLowercase]

%\renewcommand{\listoflistings}{%
%  \cleardoublepage
%  \addcontentsline{toc}{chapter}{\listoflistingscaption}%
%  \listof{listing}{\listoflistingscaption}%
%}

\newcommand{\haskell}[1]{\mintinline{haskell}{#1}}

% ------------------------------------------------------------------------------
% Hyperlinks

\usepackage{hyperref}
\definecolor{linkblue}{HTML}{0071E6}
\hypersetup{
  colorlinks  = true,
  linkcolor   = linkblue,
  urlcolor    = linkblue,
  citecolor   = linkblue,
  anchorcolor = linkblue,
}

%\renewcommand*{\backref}[1]{(Referred to on page #1.)}

\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\renewcommand{\subsubsectionautorefname}{\S}

\providecommand*{\listingautorefname}{Listing}

% ------------------------------------------------------------------------------
% Page geometry

\usepackage{geometry}
\geometry{
  hmargin        = 1.250in,
  vmargin        = 1.000in,
  marginparwidth = 0.750in,
  marginparsep   = 0.125in,
  heightrounded  = true,
}

% ------------------------------------------------------------------------------
% TikZ

\usepackage{tikz}
\usepackage[fancy]{tikz-inet}
\usetikzlibrary{%
  arrows,shapes,automata,backgrounds,petri,positioning%
}

% ------------------------------------------------------------------------------
% Short names for fonts

\newcommand{\textbs}[1]{{\sffamily\fontseries{sbc}\selectfont #1}}

\newcommand{\mathbs}[1]{\ensuremath{\text{\textbs{#1}}}}
\renewcommand{\mathtt}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\mrs}[1]{\ensuremath{\mathnormal{#1}}} % Reset font to normal
\newcommand{\mbf}[1]{\ensuremath{\mathbf{#1}}}     % Boldface
\newcommand{\mbs}[1]{\ensuremath{\mathbs{#1}}}     % Bold + sans-serif
\newcommand{\mbb}[1]{\ensuremath{\mathbb{#1}}}     % Blackboard bold
\newcommand{\mtt}[1]{\ensuremath{\mathtt{#1}}}     % Teletype
\newcommand{\mrm}[1]{\ensuremath{\mathrm{#1}}}     % Serif ("roman")
\newcommand{\msf}[1]{\ensuremath{\mathsf{#1}}}     % Sans-serif
\newcommand{\msc}[1]{\ensuremath{\mathsc{#1}}}     % Small-caps
\newcommand{\mcl}[1]{\ensuremath{\mathcal{#1}}}    % Calligraphic
\newcommand{\msr}[1]{\ensuremath{\mathscr{#1}}}    % Script
\newcommand{\mfr}[1]{\ensuremath{\mathfrak{#1}}}   % Fraktur

% ------------------------------------------------------------------------------
% Various kinds of brackets

\makeatletter
\DeclareFontFamily{OMX}{MnSymbolE}{}
\DeclareSymbolFont{MnLargeSymbols}{OMX}{MnSymbolE}{m}{n}
\SetSymbolFont{MnLargeSymbols}{bold}{OMX}{MnSymbolE}{b}{n}
\DeclareFontShape{OMX}{MnSymbolE}{m}{n}{
    <-6>  MnSymbolE5
   <6-7>  MnSymbolE6
   <7-8>  MnSymbolE7
   <8-9>  MnSymbolE8
   <9-10> MnSymbolE9
  <10-12> MnSymbolE10
  <12->   MnSymbolE12
}{}
\DeclareFontShape{OMX}{MnSymbolE}{b}{n}{
    <-6>  MnSymbolE-Bold5
   <6-7>  MnSymbolE-Bold6
   <7-8>  MnSymbolE-Bold7
   <8-9>  MnSymbolE-Bold8
   <9-10> MnSymbolE-Bold9
  <10-12> MnSymbolE-Bold10
  <12->   MnSymbolE-Bold12
}{}

\let\llangle\@undefined
\let\rrangle\@undefined
\DeclareMathDelimiter{\llangle}{\mathopen}{MnLargeSymbols}{'164}{MnLargeSymbols}{'164}
\DeclareMathDelimiter{\rrangle}{\mathclose}{MnLargeSymbols}{'171}{MnLargeSymbols}{'171}
\makeatother

\newcommand{\sbkt}[2][]{\ensuremath{{#1\llbracket{}} {#2} {#1\rrbracket{}}}}
\newcommand{\abkt}[2][]{\ensuremath{{#1\langle{}} {#2} {#1\rangle{}}}}
\newcommand{\aabkt}[2][]{\ensuremath{\llangle[#1] {#2} \rrangle[#1]}}

% ------------------------------------------------------------------------------
% TODO notes

\usepackage{todonotes}

\newlength{\fixmewidth}
\setlength{\fixmewidth}{0.7\textwidth}
\newcommand{\fixme}[1]{%
  \begin{minipage}[c]{\fixmewidth}%
  \todo[color=green!40,inline]{\textsc{fixme:} #1}%
  \end{minipage}}
\newcommand{\sfixme}[0]{%
  \begin{minipage}[c]{3.5em}%
  \todo[color=green!40,inline]{\textsc{fixme}}%
  \end{minipage}}

% ------------------------------------------------------------------------------
% Bibliography and citation style

\usepackage[backref=true]{biblatex}

\newbibmacro*{bbx:parunit}{%
  \ifbibliography{%
    \setunit{\bibpagerefpunct}{\newblock{}}%
    \usebibmacro{pageref}%
    \clearlist{pageref}%
    \setunit{\adddot\par\nobreak}}{}%
}

\renewbibmacro*{doi+eprint+url}{%
  \usebibmacro{bbx:parunit}%
  \iftoggle{bbx:doi}{\printfield{doi}}{}%
  \iftoggle{bbx:eprint}{\usebibmacro{eprint}}{}%
  \iftoggle{bbx:url}{\usebibmacro{url+urldate}}{}%
}

\renewbibmacro*{eprint}{%
  \usebibmacro{bbx:parunit}%
  \iffieldundef{eprinttype}{%
    \printfield{eprint}%
  }{%
    \printfield[eprint:\strfield{eprinttype}]{eprint}%
  }%
}

\renewbibmacro*{url+urldate}{%
  \usebibmacro{bbx:parunit}%
  \printfield{url}%
  \iffieldundef{urlyear}{
  }{%
    \setunit*{\addspace}%
    \printtext[urldate]{\printurldate}%
  }}

\addbibresource{sources.bib}

% ------------------------------------------------------------------------------
% List of definitions

% \usepackage{tocloft}
% \usepackage[english]{babel}
%
% \newcommand{\listdefinitionname}{List of Definitions}
% \newlistof{definition}{def}{\listdefinitionname}
% \newcommand{\definition}[1]{%
%   \refstepcounter{definition}
%   \par\noindent\textbf{Definition \thedefinition. #1}
%   \addcontentsline{def}{definition}
%   {\protect\numberline{\thechapter.\thedefinition}#1}\par}
%
% % \definition{Your first example}
% % \definition{Your second example}

% ------------------------------------------------------------------------------
% Miscellaneous other stuff

\newcommand{\eps}[0]{\varepsilon}

\newcommand{\catop}[1]{\ensuremath{{#1}^{\msf{op}}}}

\newcommand{\comp}[0]{\circ}
\newcommand{\tens}[0]{\otimes}

\newcommand{\example}[0]{\mathrm{example}}

\newcommand{\wrap}[1]{#1}

\DeclareMathOperator{\image}{im}
\DeclareMathOperator{\domain}{dom}

\providecommand{\tightlist}{\setlength{\itemsep}{3pt}\setlength{\parskip}{0pt}}

\newcommand{\aside}[1]{\hfill{} ({#1})}
\newcommand{\forceNewLine}[0]{{\hspace{1em}\newline{}}}
\renewcommand{\emptyset}[0]{\varnothing}

\renewcommand{\cdots}[0]{\makebox[1em][c]{${\cdot}$\hfil${\cdot}$\hfil${\cdot}$}}
\renewcommand{\dotsc}[0]{\makebox[1em][c]{.\hfil.\hfil.}}

\newlength{\stextwidth}
\newcommand{\makesamewidth}[3][c]{%
  \settowidth{\stextwidth}{#2}%
  \makebox[\stextwidth][#1]{#3}}

\newcommand{\email}[1]{\href{mailto:#1}{\texttt{#1}}}
\renewcommand{\thefootnote}{[\roman{footnote}]}

\newcommand{\naive}[0]{{na\"{\i}ve}}

% ------------------------------------------------------------------------------

% ==============================================================================
% ==============================================================================
% ==============================================================================

\begin{document}

\title{A Generic, Anytime Equality Saturation Algorithm}
\author{%
  Remy Goldschmidt \\
  \email{regolds2@illinois.edu} \\
  University of Illinois at Urbana-Champaign
}

\maketitle{}

\tableofcontents{}
\listoflistings{}
%\listofdefinition{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{sec:introduction}

Equality saturation is a framework for optimization first introduced in
a 2009 POPL paper~\cite{tate-2009} by Tate et al.
The \textit{phase ordering problem} in compiler optimization is essentially
the issue of figuring out in what order optimizations should be applied to code;
this problem is very difficult because some optimizations expose code that
allows other optimizations to be applied, while other optimizations remove
opportunities to apply optimizations. Equality saturation solves the phase
ordering problem by restructuring optimization as saturation-based automated
theorem proving (``forward chaining'') followed by combinatorial optimization.

\sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Abstract equality saturation}
\label{sec:abstract-eqsat}

The basic outline of equality saturation is that the user must first convert
a piece of code (usually a control flow graph) into a referentially transparent
directed graph with sharing, which is called a \textit{program expression graph}
(PEG)\footnotemark. Here, ``referentially transparent'' means that, assuming
there is a transition system $(S, {\to})$ representing the semantics of the
language, the semantics of a PEG node are defined purely by the node label and
the semantics of the children of that node (its ``out-neighbors''). In addition
to a PEG, equality saturation has two other inputs:

\footnotetext{
  Note that the way PEGs are defined in~\cite{tate-2009} is actually more
  specific (due to the details of optimizing imperative languages) than the
  way I will use the term in this paper; this is one of the ways in which
  this description of equality saturation is more generic than previous
  expositions.
}

\begin{enumerate}
\item {%
  A term-rewriting system on PEGs, whose rules define the basic optimizations
  that the equality saturation engine will compose together. For example, this
  term-rewriting system could simply be the operational semantics TRS of the
  programming language, in terms of PEGs (using this TRS would make the
  optimizer quite similar to a partial evaluator).
}
\item {%
  A heuristic for the runtime performance of a given PEG.
  In the most general case, this is simply a function of type
  \haskell{Term -> R} for some partially ordered semiring \haskell{R},
  which is usually an integer or floating-point type but may also be something
  more sophisticated like the symbolic integer types in the
  \href{https://hackage.haskell.org/package/sbv}{\texttt{sbv}} package.
}
\end{enumerate}

The output of equality saturation is an optimized PEG, which can then be turned
back into a control flow graph for code generation. Since equality saturation
involves a time-consuming breadth-first equational proof search, there is also
an ``anytime'' variant of equality saturation where the best known PEG so far
is emitted every so often (a user-specified \textit{timer} of type
\haskell{IO Bool} is called every time a rule is applied, and if it returns
\haskell{True}, the best PEG found so far is passed to a user-specified
\textit{callback}). Since performance is expressed as a local property (a side
effect of referential transparency), we can cache evaluations of the performance
heuristic on parts of the code that have not changed during equality saturation,
so the anytime variant of equality saturation is not asymptotically slower than
the traditional variant.

Consider the types described in \autoref{lst:basic-eqsat}. This is the most
basic abstract description of equality saturation. The intended semantics of the
\texttt{saturate} function are the following:

\begin{enumerate}
\item Convert the given term to a PEG, and then to an EPEG.
\item Apply a rule to a node.
\item {%
  Call the ``timer'':
  \begin{enumerate}
  \item {%
    If it returns \texttt{True}, select the best sub-PEG using the heuristic and
    call the callback with this PEG (and its quality and equality proof).
  }
  \item {%
    Otherwise, go to step 2.
  }
  \end{enumerate}
}
\end{enumerate}

\begin{listing}[ht]
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type Variable = Natural

data TermRepr = G | T

data Term (repr :: TermRepr) (node :: *) (var :: *) where
  Ref  :: Natural                      -> Term 'G   node var
  Var  :: var                          -> Term repr node var
  Node :: (node, [Term repr node var]) -> Term repr node var

type OpenTTerm   node = Term 'T node Variable
type OpenGTerm   node = Term 'G node Variable
type ClosedTTerm node = Term 'T node Void
type ClosedGTerm node = Term 'G node Void

-- Any Term can be converted to a GTerm, but not vice versa.
upcastTerm :: Term repr node var -> GTerm node var

-- Invariant: variables in right term are a subset of variables in left term
type Equation node = (TTerm node Variable, GTerm node Variable)

type TRS  node       = Set (Equation node)
type LTRS node label = Map (Equation node) label

type TermCursor = [Natural]

data Proof label = QED | ApplyRule TermCursor label (Proof label)

type Heuristic m node real = (Term node -> m real)

type Timer m = m Bool

type Callback m node real label
  = ( Term node   -- The best term discovered so far
    , real        -- The quality of this term
    , Proof label -- A proof of equality with the original term
    ) -> m Bool   -- Returning False here will quit saturation

saturate :: (Ord real, Monad m)
         => LTRS node label            -- The rewrite rules to optimize with
         -> Heuristic m node real      -- The performance heuristic
         -> Term node                  -- The term to optimize
         -> Timer m                    -- The "timer"
         -> Callback m node real label -- The "callback"
         -> m ()
\end{minted}
\caption{The most basic exposition of equality saturation}
\label{lst:basic-eqsat}
\end{listing}

%The laws of the \haskell{}

\fixme{explain more about eqsat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving performance}
\label{sec:improving-performance}

\fixme{introduction paragraph}

\section{Avoiding intermediate data structures}
\label{sec:avoiding-intermediates}

\sfixme{}

\section{Maximal sharing}
\label{sec:msharing}

The version of equality saturation I have shown you thus far has a major issue:
it doesn't account for sharing in the EPEG. We can solve this via hash-consing,
although the presence of cycles in the EPEG presents a problem for most
hash-consing algorithms. The solution is to compute the set of
strongly-connected components (SCCs) of the EPEG, and then hash-cons the graph
the nodes of which are pairs of EPEG nodes and the SCCs they belong to and
the edges of which are a spanning tree of the EPEG. Pairing the nodes with their
SCCs ensures that the cycles are included in the hash of each node, and an EPEG
can be recovered by reintroducing the cycles from the SCCs.

\section{Cycle detection}
\label{sec:cycle-detection}

Consider any term rewriting system with a rule like $f(x, y) \mapsto f(y, x)$
for some function symbol $f$; it is clear that such a rewrite rule will cause
a \naive{} implementation of equality saturation to run forever on any term
including $f$, since the rule can always be applied. For computability-theoretic
reasons it is impossible to prevent all such loops, but in many cases we can
prevent loops in the following way:

\begin{itemize}
\item {%
  If there are $n$ rules in the term rewriting system, associate a bitvector of
  length $n$, filled with zeros, to each node in the EPEG. For a node $A$, we
  will denote this bitvector by $v_A \in \{0, \ldots, n - 1\} \to \mbs{2}$.
}
\item {%
  When we merge nodes $A$ and $B$ during hash-consing into a node $C$, it should
  be the case that
  $ v_C
  = {\lor}_\msf{bitwise}(v_A, v_B)
  = \{(i, v_A(i) \lor v_B(i)) \mid i \in \{0, \ldots, n - 1\}\}$.
}
\item {%
  If the $i$th rule in the term rewriting system is going to be applied to
  node $A$, first check whether $v_A(i) = 1$. If so, then skip applying this
  rule. Otherwise, set the value of $v_A(i)$ to $1$ and apply the rule.
}
\end{itemize}

Unfortunately, as the number of rules increases, this solution becomes slower
and slower, as the bitvectors will tend to be sparse (mostly zeros). Ideally,
an adaptive data structure would be used that is the same as a bitvector as
long as the size of the bitvector is smaller than that of a cache line, and once
this limit is reached, it becomes a trie or hashset. Since the number of rules
is completely known as soon as equality saturation begins, such an adaptive data
structure need not have any runtime overhead relative to its underlying data
structures.

\section{Online SCC computation}
\label{sec:online-scc}

The sharing algorithm described in \autoref{sec:msharing} requires recomputation
of the strongly-connected component graph of the EPEG every time a node is
added. This is inefficient, since most of the work done in each SCC computation
will be similar before and after the addition of a node. This inefficiency can
be prevented by using any of the online algorithms for SCC computation. As of
this writing, the state of the art for online SCC computation is described in
a 2015 paper~\cite{bender-2015} by Bender et al. There are two algorithms
defined in the paper by Bender: one that is optimal for sparse graphs, and one
that is optimal for dense graphs. It is not clear to me which algorithm is more
relevant to equality saturation, since although it seems at first glance that
our graphs are sparse, the whole point of an EPEG is to maximize sharing, so
it may be that the graphs end up being dense. This will probably require
empirical study; it is likely that constant factors are a bigger concern than
the asymptotics of these two algorithms.

\section{Term indexing}
\label{sec:term-indexing}

\textit{Term indexing} is a field of study in which the problem of compactly
storing a large number of open terms such that the index can be queried for a
subset that includes the set of added terms that will unify with a query term
\cite{handbook-ch26}. A term index is best described by the typeclass shown in
\autoref{lst:term-index}.

A term index effectively acts as a filter that can be applied before matching;
it allows false positives, but does not allow false negatives, much like a
Bloom filter~\cite{bloom-1970}. By filtering the set of patterns before we do
pattern matching, we can decrease the amount of work that needs to be done
during pattern matching. A term index is a \textit{perfect filter} if it never
returns a false positive; otherwise it is an \textit{imperfect filter}.

\begin{listing}[ht]
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type Key node var = (Ord node, Ord var)
class TermIndex (index :: * -> * -> * -> *) where
  -- An injective type family defining the mutable version of an immutable
  -- term index. The extra s parameter is for an ST-style state token.
  type Mut index (node :: *) (var :: *) (value :: *) (s :: *)
    = (result :: *) | result -> index

  empty        :: index node var value
  freeze       :: Mut index node var value s
               -> ST s (index node var value)
  thaw         :: index node var value
               -> ST s (Mut index node var value s)
  insertMany   :: (Key node var)
               => Mut index node var value s
               -> [(TTerm node var, value)]
               -> ST s ()
  queryMany    :: (Key node var, Monad m)
               => index node var value
               -> [(TTerm node var, value -> m any)]
               -> m ()
  queryManyMut :: (Key node var)
               => Mut index node var value s
               -> [(TTerm node var, value -> ST s any)]
               -> ST s ()
\end{minted}
\caption{A typeclass describing a term index}
\label{lst:term-index}
\end{listing}

Every \haskell{TermIndex} has a mutable (\haskell{Mut}) version and an immutable
version. The \haskell{freeze} and \haskell{thaw} methods allow interconversion
between these types. The \haskell{empty} method is the only way to create a term
index from nothing. The laws of the \haskell{TermIndex} typeclass are:

\vspace{-1em}
\begin{align}
  \mtt{\haskell{thaw} \haskell{>=>} \haskell{freeze}}
  & = \haskell{pure}
  \label{eq:term-index-law-1} \\
  \mtt{\haskell{insertMany} $i$ \haskell{[]}}
  & = \haskell{pure ()}
  \label{eq:term-index-law-2} \\
  \mtt{\haskell{insertMany} $i$ ($x \mathbin{\haskell{++}} y$)}
  & = (\mtt{\haskell{insertMany} $i$ $x$ \haskell{>>} \haskell{insertMany} $i$ $y$})
  \label{eq:term-index-law-3} \\
  \mtt{\haskell{queryMany} $i$ \haskell{[]}}
  & = \haskell{pure ()}
  \label{eq:term-index-law-4} \\
  \mtt{\haskell{queryMany} $i$ $(x \mathbin{\haskell{++}} y)$}
  & = (\mtt{\haskell{queryMany} $i$ $x$ \haskell{>>} \haskell{queryMany} $i$ $y$})
  \label{eq:term-index-law-5} \\
  \mtt{\haskell{queryMany} \haskell{empty} $q$}
  & = \haskell{pure ()}
  \label{eq:term-index-law-6} \\
  \mtt{\haskell{queryManyMut} $i_m$ $q$}
  & = (\mtt{\haskell{freeze} $i_m$}
    \:\: \haskell{>>=} \:\:
    (\lambda i \, \mapsto \, \mtt{\haskell{queryMany} $i$ $q$}))
  \label{eq:term-index-law-7}
\end{align}

The reason I define the fundamental insertion and querying operations of a
\haskell{TermIndex} as \haskell{insertMany} and \haskell{queryMany}, rather
than as

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
class TermIndex (index :: * -> * -> * -> *) where
  -- ... other definitions ...
  insert :: (Key n v) => Mut index n v val s -> TTerm n v -> val -> ST s ()
  query  :: (Key n v, Monad m) => index n v val -> TTerm n v -> m [val]
\end{minted}
\vspace{0em}

is for two reasons. Firstly, there are some term indexing data structures that
are more efficient when you are inserting or querying multiple terms at the same
time, and since \haskell{insert} and \haskell{query} can be defined in terms of
\haskell{insertMany} and \haskell{queryMany} relatively easily, it makes more
sense for \haskell{insertMany} and \haskell{queryMany} to be the primitive
operations. Secondly, by having \haskell{queryMany} accept a monadic callback
rather than returning a list of results, we can avoid the creation of
intermediate data structures.

While it is possible to define \haskell{insert} and \haskell{query} in terms of
\haskell{insertMany} and \haskell{queryMany} respectively, it causes an
additional allocation and pointer indirection for the singleton list, so in an
actual version of the \haskell{TermIndex} typeclass, \haskell{insert} and
\haskell{query} would be defined as methods with default implementations.
For additional ease of defining instances, \haskell{insertMany} and
\haskell{queryMany} can also have default implementations in terms of
\haskell{insert} and \haskell{query} respectively, and to avoid people writing
empty instances (which would loop infinitely), a \texttt{MINIMAL} pragma can be
added, which will allow GHC to check that a valid set of methods have been
defined.

\subsection{Discrimination trees}
\label{sec:discrimination-trees}

A \textit{discrimination tree} is a term-indexing data structure based on a
trie~\cite{handbook-ch26}. Define the \textit{path} of a term
\haskell{t :: TTerm n v} to be a value of type \haskell{[Maybe n]} representing
the preorder traversal of the term, replacing any variable nodes with
\haskell{Nothing}. Inserting a term-value-pair into a discrimination tree is the
same as inserting the path computed from the term along with the value into a
trie of type \haskell{Trie (Maybe n) val}. Querying a discrimination tree for a
given term \haskell{q :: TTerm n v} is the same as computing the path of
\haskell{q} and doing a backtracking search through the trie for matching paths;
this is much the same as the traditional trie retrieval algorithm, except that
when the \haskell{Maybe n} value you're searching for at a node is
\haskell{Nothing}, you nondeterministically search in all the the children, and
when the value you're searching for at a node is \haskell{Just n}, you
nondeterministically search in the \haskell{n} and \haskell{Nothing} children.

Note that discrimination trees only work when your terms don't include variadic
function symbols. If they do include variadic function symbols, you can still
use discrimination trees by simply augmenting your node type with the number of
children at that node, effectively introducing one function symbol for every
possible variadic usage.

For cache efficiency, it is ideal to implement a discrimination tree using a
trie based on a contiguous growable array (and the children of each node should
be referenced with \haskell{Word32} offsets rather than pointers, since in
practice the tree will rarely exceed $\SI{4}{\gibi\byte}$).

\subsection{Substitution trees}
\label{sec:substitution-trees}

A \textit{substitution tree} is a term-indexing data structure based on a tree
of substitutions with values at leaf nodes~\cite{graf-1994,handbook-ch26}.
The intended semantics of such a tree is that each substitution in composed
with the substitutions above it to compute a filter for terms.

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type Sub node var = Map var (TTerm node var)

domain :: Sub node var -> Set var

type Indicator = Int

data SubTree node var value
  = Leaf value
  | Branch (Sub node (Either Indicator var)) [SubTree node var value]
\end{minted}
\vspace{0em}

There are three invariants that must be true for the \haskell{SubTree} type,
which may be enforced by making \haskell{SubTree} an abstract type in its own
module. Firstly, if the size of the list of children at a \haskell{Branch} node
is $n \in \mbb{N}$, then it must always be true that $n \neq 1$. Secondly, for
every path
\texttt{[(\haskell{σ₁}, \haskell{ω₁}), …, (\haskell{σₙ}, \haskell{ωₙ})] \haskell{::} \haskell{[Sub n (Either Indicator v), [SubTree n v val]]}}
from the root to a leaf of a non-empty tree it must be the case that the
variables introduced by the composition $\haskell{ωₙ} \comp \ldots \comp \haskell{ω₁}$ are
all \texttt{\haskell{Left} \haskell{i}} for some \haskell{i :: Indicator}.
Thirdly, for every such path and every $j \in \{1, \ldots, n - 1\} \subset \mbb{N}$,
it must be the case that, when we define $k = j + 1$,
\texttt{\haskell{null} (\haskell{domain} \haskell{σₖ} ∩ (\haskell{domain} \haskell{σ₀} ∪ … ∪ \haskell{domain} \haskell{σⱼ})) \haskell{==} \haskell{True}}.

Querying a substitution tree for a term \haskell{q :: TTerm n v} involves doing
a backtracking search; \sfixme{}.

\section{Caching performance heuristic executions}
\label{sec:heuristic-caching}

One of the disadvantages of the anytime version of equality saturation described
thus far is that the performance heuristic must be run on every node of the
EPEG every time the timer returns \haskell{True}. For a large class of
performance heuristics, it turns out that this is not actually necessary
however. Since the semantics of a PEG/EPEG node are defined solely by the node
contents and the children of that node, any performance heuristic that is
definable solely in terms of the semantics of a node can be expressed as a
\textit{catamorphism}; a function of type \haskell{(node, [real]) -> real} that
is folded over the tree. A sufficient but not necessary condition for a
heuristic being definable in this way is if the definition of semantics is
purely operational in nature; this is true in most strictly evaluated languages
but not in many non-strict languages.

If a heuristic is definable as a catamorphism, then we can cache heuristic
executions by noticing that any subtree of the EPEG that is unchanged between
the current EPEG and the last execution of \haskell{selectBest} \sfixme{}
will have the same best sub-PEG as the last execution.

We can take this further, however. If we know that we have saturated a subtree
with equalities (e.g.: since the cycle-detection algorithm described in
\autoref{sec:cycle-detection} collects information on what rules have been
applied to a given node, we can propagate a token that represents the fact that
a given subtree is saturated), then we can entirely throw out all the other
(non-optimal) sub-PEGs in that subtree. Note that when we throw away the
non-optimal sub-PEGs, we don't reset the cycle-detection data structures
associated with each node, thereby preventing the saturation algorithm from
exploring already-explored parts of the search space.

This may cause repeated work if rewriting ends up generating the same code,
which we can prevent by ``axiomatizing'' the optimization; introducing a
(derivable) rewrite rule with ground terms on both sides to the primitive
term-rewriting system used for equality saturation. This added axiom may have to
be specially marked so that the cycle-detection data structure gets initialized
correctly after applying it.

It is unclear to me whether any of these ideas will actually create speed
improvements in practice, but they are probably worth exploring.

\section{Parallelization}
\label{sec:parallelization}

Since equality saturation is, at its heart, a breadth-first search, it seems
quite amenable to parallelization. However, the maximal sharing algorithm
described in \autoref{sec:msharing} creates a problem; worker threads cannot
work independently on different parts of the graph without mutexes if sharing
might combine these independent sections. There are several possible solutions
to this, but the simplest to implement is that the maximal sharing algorithm
should run independently of the worker threads, locking the entire graph while
deduplication occurs. In this exposition of equality saturation, we need to
freeze the entire graph anyway to capture the corresponding \haskell{TTerm} to
be passed to the callback, so this seems like not a huge loss, but there may be
a way to implement parallel anytime equality saturation with less time devoted
to blocking.

\section{Automatically-applied rules}
\label{sec:automatic-rules}

\sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving expressiveness}
\label{sec:improving-expressiveness}

\fixme{introduction paragraph}

\section{Variadic function symbols}
\label{sec:variadic-function-symbols}

Many languages contain constructors that are in some sense variadic.
This variadicity can be eliminated by adding a constructor to the \haskell{Term}
type that allows for variadic function symbols. This would allow term indexing
data structures like discrimination trees, which do not ordinarily handle
variadic function symbols, to be expressed in a more type-safe way.

\section{Adding equational axioms}
\label{sec:equational-axioms}

Many languages contain constructors that should be considered equal under some
non-syntactic equivalence relation. For example, if you have multiple
independent modules in the code you are optimizing, then cross-module inlining
is only possible if the constructor for a set of modules is actually considered
modulo associativity and commutativity. You can, of course, add rewrite rules
to your theory corresponding to associativity and commutativity, but in many
cases this will be much slower (due to spurious copying) than a dedicated solver
for associative-commutative matching. There are a number of theories that can
be integrated into the matching algorithm, including:

\begin{itemize}
\item Semigroups (\textsc{a})
\item Abelian semigroups (\textsc{ac})
\item Abelian monoids (\textsc{acu})
\item Idempotent abelian semigroups (\textsc{aci})
\item Boolean rings (\textsc{br})
\item Abelian groups (\textsc{ag})
\end{itemize}

For a relatively comprehensive review of the literature on equational matching
and unification, see \cite{siekmann-1989}. Further research is needed on the
asymptotic and in-practice complexity of these matching algorithms to determine
which ones are relevant to a practical implementation of equality saturation,
but given that the $\mbb{K}$ Framework implements \textsc{ac}-unification, it is
likely that this at least is worth implementing.

\section{Conditional rewriting}
\label{sec:conditional-rewriting}

Many optimizations cannot easily be implemented using a term-rewriting system
alone; it is often much easier to write optimizations in a \textit{conditional}
term-rewriting system, in which rules can have boolean side conditions.

We can improve the expressiveness of our equality saturation algorithm by
implementing conditional rewriting. One efficient way to implement this is by
eliminating a conditional TRS into an equivalent unconditional TRS using the
algorithm described in \cite{rosu-2005}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving correctness}
\label{sec:improving-correctness}

\fixme{introduction paragraph}

\section{Type systems}
\label{sec:type-systems}

\sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Integration with other tools}
\label{sec:integration}

\fixme{introduction paragraph}

\section{The $\mathbb{K}$ Framework}
\label{sec:k-framework}

Citation: \cite{rosu-2010}

\sfixme{}

\section{$\mathrm{IncA}$ / $\mathrm{IncA}_L$}
\label{sec:inca-incal}

Citation: \cite{szabo-2016}, \cite{szabo-2017}

\sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Example IR formats}
\label{sec:example-languages}

\fixme{introduction paragraph}

\section{Cartesian closed categories}
\label{sec:cccs}

Citation: \cite{elliott-2017}

\sfixme{}

\section{Interaction nets}
\label{sec:interaction-nets}

Citation: \cite{asperti-1998}

\sfixme{}

\section{Term-rewriting systems}
\label{sec:term-rewriting-systems}

\sfixme{}

% Adithya Murali, Alexander Altman, Remy Goldschmidt

\section{Relational algebra}
\label{sec:relational-algebra}

Relational algebra, which is a mathematical formalization of a fragment of the
languages (like SQL) commonly used for querying relational databases, seems like
an ideal language to optimize with equality saturation. It has many equational
laws and is already completely referentially transparent.

The same is not true of Datalog, a non-Turing complete fragment of Prolog that
is more expressive than relational algebra, but there may be a way to express
Datalog queries in a referentially transparent syntax; I suspect it may look
like augmenting relational algebra with some kind of fixed-point operator, but
I am not sure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Future Work}
\label{sec:future-work}

\fixme{introduction paragraph}

\section{Incremental equality saturation}
\label{sec:incremental-eqsat}

If there is an algorithm described by a function \haskell{f :: A -> B}, we say
that \haskell{f' :: (A, ΔA) -> B} is the \textit{incrementalized} version of
\haskell{f} if \haskell{ΔA} is a monoid and there exists a monoid action
\haskell{patch :: ΔA -> A -> A} such that for any \haskell{x :: A} and
\haskell{δ :: ΔA},
\texttt{\haskell{f'} (\haskell{x}, \haskell{δ}) = \haskell{f} (\haskell{patch} \haskell{δ} \haskell{x})}.
Ideally, if \haskell{x} is already computed and \haskell{δ} is ``small'', the
incrementalized version \texttt{\haskell{f'} (\haskell{x}, \haskell{δ})} will
also be faster to compute than
\texttt{\haskell{f} (\haskell{patch} \haskell{δ} \haskell{x})},
though this connotation is hard to formalize, especially in a
lazy language like Haskell.

We can likely usefully incrementalize equality saturation in this sense, as
referential transparency ensures that we only need to re-optimize the parts of
the PEG that have changed.

This would probably look something like:

\begin{enumerate}
\item {%
  Compute the difference of the old and new PEGs using
  a modified version of \cite{lempsink-2009}.
}
\item {%
  If there are common subtrees, apply the proofs from the old EPEG
  to the new EPEG.
}
\item {%
  Exclude the already-explored proof tree branches in the new EPEG.
}
\end{enumerate}

Alternatively, we may simply be able to reuse the pre-existing EPEG by adding
the nodes of the new PEG to optimize to the EPEG, running the maximal sharing
algorithm on it, and removing all nodes that are not in the connected component
containing the root node of that PEG.

This feature seems especially important for real-world compilers using equality
saturation, as it could drastically reduce build times when optimization is
enabled and small changes have been made. There are plenty of unexplored
questions in this realm; for example, how should the EPEG be quickly and
compactly serialized to avoid overhead due to this feature?

\section{Dependent types}
\label{sec:dependent-types}

Code written in languages with dependent type systems often ends up containing
many theorems and equations encoded in higher-order logic at the type level.
These equations could be useful for equality saturation, as they represent
(often nontrivial) ways of rewriting a program. Selsam and Moura wrote a 2016
paper \cite{selsam-2017} on congruence closure in intensional type theory that
seems especially relevant to an implementation of equality saturation for a
dependently-typed language.

\section{Homotopy type theory}
\label{sec:hott}

Homotopy type theory is a constructive foundation for mathematics based on
intensional type theory with higher inductive types and the univalence axiom,
which is incompatible with the \textit{uniqueness of identity proofs} axiom
commonly assumed in dependent type theory.

The main thing distinguishing proofs in homotopy type theory from proofs in
dependently typed languages is the fact that there can be nontrivial equalities
between type-level equalities. In fact, types in homotopy type theory can
usefully be thought of as weak $\omega$-groupoids, which can have arbitrarily
tall towers of nontrivial equality. Cubical type theory is an experimental
computable implementation of homotopy type theory, so homotopy type theory may
have relevance to computer applications.

It would be interesting to explore saturation-based automated theorem proving
in the context of these weak $\omega$-groupoids, rather than the $1$-groupoids
(equivalence relations) we've considered in this paper.

% \section{Laziness}
% \label{sec:laziness}
%
% Citation: \cite{okasaki-1998}
%
% \sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{easylist}[itemize]
% @ Equality saturation
% @@ \cite{tate-2009}
% @@ \cite{tate-2012-eqsat}
% @@ \cite{stepp-2011}
% @ Proof generalization
% @@ \cite{tate-2012-proofgen}
% @ Term indexing
% @@ \cite{handbook-ch26}
% @@ \cite{graf-1994}
% @@ \cite{holen-2013}
% @ Unification / Matching
% @@ \cite{baader-2015}
% @@ \cite{plotkin-1972}
% @@ \cite{eker-2003}
% @@ \cite{eker-1995}
% @@ \cite{forgy-1979}
% @@ \cite{doorenbos-2001}
% @ Resource Estimation
% @@ \cite{carbonneaux-2015}
% @@ \cite{hoffmann-2012}
% @ Static Analysis
% @@ \cite{calcagno-2011}
% @@ \cite{brotherston-2017}
% @ SMT solving
% @@ \cite{bjorner-2015}
% @ Reference
% @@ \cite{baader-1998}
% @ Miscellaneous
% @@ \cite{alpuente-2014}
% @@ \cite{lewenstein-2013}
% @@ \cite{stampoulis-2010}
% @@ \cite{sjoberg-2014}
% @@ \cite{bachmair-2003}
% @@ \cite{payet-2008}
% @@ \cite{baumgartner-2007}
% @@ \cite{darais-2017}
% \end{easylist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[heading=bibnumbered]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
