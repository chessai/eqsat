\documentclass[11pt]{report}

% ------------------------------------------------------------------------------
% Dependencies

%\usepackage{cmbright}             % Computer Modern Bright
%\usepackage{eucal}                % Euler (calligraphic/script)
\usepackage{bm}                    % Bold math font
\usepackage[at]{easylist}          % Easy-to-use lists
\usepackage{enumitem}              % Customize `enumerate' lists
\usepackage{amsmath}               % The AMS math package
\usepackage{amssymb}               % The AMS symbols package
\usepackage{mathtools}             % Extensions to `amsmath'
\usepackage{wasysym}               % Various symbols, including smiley faces
\usepackage{stmaryrd}              % Logic and CS symbols
\usepackage{xfrac}                 % Split level fractions with \sfrac
\usepackage{bussproofs}            % Gentzen-style inference rules
\usepackage{fontspec}              % Fonts
\usepackage{listings}              % Code listings
\usepackage{color}                 % Color
\usepackage{xcolor}                % Color
\usepackage[binary-units]{siunitx} % SI units
\usepackage{parskip}

\usepackage[chapter]{tocbibind}

% ------------------------------------------------------------------------------
% Pseudocode

%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}

% ------------------------------------------------------------------------------
% Watermarks

%\usepackage{draftwatermark}
%\SetWatermarkText{Draft}
%\SetWatermarkScale{2}
%\SetWatermarkColor[gray]{0.95}

% ------------------------------------------------------------------------------
% Chapter titles

\usepackage{titlesec}
\definecolor{gray75}{gray}{0.75}
\titleformat{\chapter}[hang]{\Huge\bfseries}{%
  \thechapter\hspace{20pt}\textcolor{gray75}{|}\hspace{20pt}%
}{0pt}{\Huge\bfseries}
\titlespacing*{\chapter}{0pt}{-20pt}{40pt}

% ------------------------------------------------------------------------------
% Code blocks

\usepackage[outputdir=../out,chapter]{minted}

\usemintedstyle[haskell]{trac}

\setmonofont{DejaVu Sans Mono}[Scale=MatchLowercase]

%\renewcommand{\listoflistings}{%
%  \cleardoublepage
%  \addcontentsline{toc}{chapter}{\listoflistingscaption}%
%  \listof{listing}{\listoflistingscaption}%
%}

\newcommand{\haskell}[1]{\mintinline{haskell}{#1}}

% ------------------------------------------------------------------------------
% Hyperlinks

\usepackage{hyperref}
\definecolor{linkblue}{HTML}{0071E6}
\hypersetup{
  colorlinks  = true,
  linkcolor   = linkblue,
  urlcolor    = linkblue,
  citecolor   = linkblue,
  anchorcolor = linkblue,
}

%\renewcommand*{\backref}[1]{(Referred to on page #1.)}

\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\renewcommand{\subsubsectionautorefname}{\S}

\providecommand*{\listingautorefname}{Listing}

% ------------------------------------------------------------------------------
% Page geometry

\usepackage{geometry}
\geometry{
  hmargin        = 1.250in,
  vmargin        = 1.000in,
  marginparwidth = 0.750in,
  marginparsep   = 0.125in,
  heightrounded  = true,
}

% ------------------------------------------------------------------------------
% TikZ

\usepackage{tikz}
\usepackage[fancy]{tikz-inet}
\usetikzlibrary{%
  arrows,cd,shapes,automata,backgrounds,petri,positioning%
}

% ------------------------------------------------------------------------------
% Short names for fonts

\newcommand{\textbs}[1]{{\sffamily\fontseries{sbc}\selectfont #1}}

\newcommand{\mathbs}[1]{\ensuremath{\text{\textbs{#1}}}}
\renewcommand{\mathtt}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\mrs}[1]{\ensuremath{\mathnormal{#1}}} % Reset font to normal
\newcommand{\mbf}[1]{\ensuremath{\mathbf{#1}}}     % Boldface
\newcommand{\mbs}[1]{\ensuremath{\mathbs{#1}}}     % Bold + sans-serif
\newcommand{\mbb}[1]{\ensuremath{\mathbb{#1}}}     % Blackboard bold
\newcommand{\mtt}[1]{\ensuremath{\mathtt{#1}}}     % Teletype
\newcommand{\mrm}[1]{\ensuremath{\mathrm{#1}}}     % Serif ("roman")
\newcommand{\msf}[1]{\ensuremath{\mathsf{#1}}}     % Sans-serif
\newcommand{\msc}[1]{\ensuremath{\mathsc{#1}}}     % Small-caps
\newcommand{\mcl}[1]{\ensuremath{\mathcal{#1}}}    % Calligraphic
\newcommand{\msr}[1]{\ensuremath{\mathscr{#1}}}    % Script
\newcommand{\mfr}[1]{\ensuremath{\mathfrak{#1}}}   % Fraktur

% ------------------------------------------------------------------------------
% Various kinds of brackets

\makeatletter
\DeclareFontFamily{OMX}{MnSymbolE}{}
\DeclareSymbolFont{MnLargeSymbols}{OMX}{MnSymbolE}{m}{n}
\SetSymbolFont{MnLargeSymbols}{bold}{OMX}{MnSymbolE}{b}{n}
\DeclareFontShape{OMX}{MnSymbolE}{m}{n}{
    <-6>  MnSymbolE5
   <6-7>  MnSymbolE6
   <7-8>  MnSymbolE7
   <8-9>  MnSymbolE8
   <9-10> MnSymbolE9
  <10-12> MnSymbolE10
  <12->   MnSymbolE12
}{}
\DeclareFontShape{OMX}{MnSymbolE}{b}{n}{
    <-6>  MnSymbolE-Bold5
   <6-7>  MnSymbolE-Bold6
   <7-8>  MnSymbolE-Bold7
   <8-9>  MnSymbolE-Bold8
   <9-10> MnSymbolE-Bold9
  <10-12> MnSymbolE-Bold10
  <12->   MnSymbolE-Bold12
}{}

\let\llangle\@undefined
\let\rrangle\@undefined
\DeclareMathDelimiter{\llangle}{\mathopen}{MnLargeSymbols}{'164}{MnLargeSymbols}{'164}
\DeclareMathDelimiter{\rrangle}{\mathclose}{MnLargeSymbols}{'171}{MnLargeSymbols}{'171}
\makeatother

\newcommand{\sbkt}[2][]{\ensuremath{{#1\llbracket{}} {#2} {#1\rrbracket{}}}}
\newcommand{\abkt}[2][]{\ensuremath{{#1\langle{}} {#2} {#1\rangle{}}}}
\newcommand{\aabkt}[2][]{\ensuremath{\llangle[#1] {#2} \rrangle[#1]}}

% ------------------------------------------------------------------------------
% TODO notes

\usepackage{todonotes}

\newlength{\fixmewidth}
\setlength{\fixmewidth}{0.7\textwidth}
\newcommand{\fixme}[1]{%
  \begin{minipage}[c]{\fixmewidth}%
  \todo[color=green!40,inline]{\textsc{fixme:} #1}%
  \end{minipage}}
\newcommand{\sfixme}[0]{%
  \begin{minipage}[c]{3.5em}%
  \todo[color=green!40,inline]{\textsc{fixme}}%
  \end{minipage}}

% ------------------------------------------------------------------------------
% Bibliography and citation style

\usepackage[backref=true]{biblatex}

\newbibmacro*{bbx:parunit}{%
  \ifbibliography{%
    \setunit{\bibpagerefpunct}{\newblock{}}%
    \usebibmacro{pageref}%
    \clearlist{pageref}%
    \setunit{\adddot\par\nobreak}}{}%
}

\renewbibmacro*{doi+eprint+url}{%
  \usebibmacro{bbx:parunit}%
  \iftoggle{bbx:doi}{\printfield{doi}}{}%
  \iftoggle{bbx:eprint}{\usebibmacro{eprint}}{}%
  \iftoggle{bbx:url}{\usebibmacro{url+urldate}}{}%
}

\renewbibmacro*{eprint}{%
  \usebibmacro{bbx:parunit}%
  \iffieldundef{eprinttype}{%
    \printfield{eprint}%
  }{%
    \printfield[eprint:\strfield{eprinttype}]{eprint}%
  }%
}

\renewbibmacro*{url+urldate}{%
  \usebibmacro{bbx:parunit}%
  \printfield{url}%
  \iffieldundef{urlyear}{
  }{%
    \setunit*{\addspace}%
    \printtext[urldate]{\printurldate}%
  }}

\addbibresource{sources.bib}

% ------------------------------------------------------------------------------
% List of definitions

% \usepackage{tocloft}
% \usepackage[english]{babel}
%
% \newcommand{\listdefinitionname}{List of Definitions}
% \newlistof{definition}{def}{\listdefinitionname}
% \newcommand{\definition}[1]{%
%   \refstepcounter{definition}
%   \par\noindent\textbf{Definition \thedefinition. #1}
%   \addcontentsline{def}{definition}
%   {\protect\numberline{\thechapter.\thedefinition}#1}\par}
%
% % \definition{Your first example}
% % \definition{Your second example}

% ------------------------------------------------------------------------------
% Miscellaneous other stuff

\newcommand{\eps}[0]{\varepsilon}

\newcommand{\catop}[1]{\ensuremath{{#1}^{\msf{op}}}}

\newcommand{\comp}[0]{\circ}
\newcommand{\tens}[0]{\otimes}

\newcommand{\example}[0]{\mathrm{example}}

\newcommand{\wrap}[1]{#1}

\DeclareMathOperator{\image}{im}
\DeclareMathOperator{\domain}{dom}

\providecommand{\tightlist}{\setlength{\itemsep}{3pt}\setlength{\parskip}{0pt}}

\newcommand{\aside}[1]{\hfill{} ({#1})}
\newcommand{\forceNewLine}[0]{{\hspace{1em}\newline{}}}
\renewcommand{\emptyset}[0]{\varnothing}

\renewcommand{\cdots}[0]{\makebox[1em][c]{${\cdot}$\hfil${\cdot}$\hfil${\cdot}$}}
\renewcommand{\dotsc}[0]{\makebox[1em][c]{.\hfil.\hfil.}}

\newlength{\stextwidth}
\newcommand{\makesamewidth}[3][c]{%
  \settowidth{\stextwidth}{#2}%
  \makebox[\stextwidth][#1]{#3}}

\newcommand{\email}[1]{\href{mailto:#1}{\texttt{#1}}}
\renewcommand{\thefootnote}{[\roman{footnote}]}

\newcommand{\hackage}[1]{%
  \href{https://hackage.haskell.org/package/#1}{\texttt{#1}}}

\newcommand{\naive}[0]{{na\"{\i}ve}}

% ------------------------------------------------------------------------------

% ==============================================================================
% ==============================================================================
% ==============================================================================

\begin{document}

\title{A Generic, Anytime Equality Saturation Algorithm}
\author{%
  Remy Goldschmidt \\
  \email{regolds2@illinois.edu} \\
  University of Illinois at Urbana-Champaign
}

\maketitle{}

\renewcommand{\baselinestretch}{0.95}\normalsize
\tableofcontents{}
\renewcommand{\baselinestretch}{1.0}\normalsize
%\listoflistings{}
%\listofdefinition{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{sec:introduction}

Equality saturation is a framework for optimization first introduced in
a 2009 POPL paper~\cite{tate-2009} by Tate et al.
The \textit{phase ordering problem} in compiler optimization is essentially
the issue of figuring out in what order optimizations should be applied to code;
this problem is very difficult because some optimizations expose code that
allows other optimizations to be applied, while other optimizations remove
opportunities to apply optimizations. Equality saturation solves the phase
ordering problem by restructuring optimization as saturation-based automated
theorem proving (``forward chaining'') followed by combinatorial optimization.

The equality saturation architecture described in this paper is currently being
implemented in an open source Apache 2.0 licensed project hosted on GitHub:
\url{https://github.com/taktoa/eqsat}.

Note that although we often use the Haskell list type constructor \haskell{[]}
in this paper, a real implementation of equality saturation would likely use
\haskell{Vector} or \haskell{Seq} from the \hackage{vector} and
\hackage{containers} libraries respectively. In general, this paper will focus
on describing algorithms in a specific-but-unperformant way, followed by a
vague-but-performant descripton.

%\fixme{write more in this section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Abstract equality saturation}
\label{sec:abstract-eqsat}

The basic outline of equality saturation is that the user must first convert
a piece of code (usually a control flow graph) into a referentially transparent
directed graph with sharing, which is called a \textit{program expression graph}
(PEG)\footnotemark. Here, ``referentially transparent'' means that, assuming
there is a transition system $(S, {\to})$ representing the semantics of the
language, the semantics of a PEG node are defined purely by the node label and
the semantics of the children of that node (its ``out-neighbors''). In addition
to a PEG, equality saturation has two other inputs:

\footnotetext{
  Note that the way PEGs are defined in~\cite{tate-2009} is actually more
  specific (due to the details of optimizing imperative languages) than the
  way I will use the term in this paper; this is one of the ways in which
  this description of equality saturation is more generic than previous
  expositions.
}

\vspace{0.5em}

\begin{enumerate}
\item {%
  A term-rewriting system on PEGs, whose rules define the basic optimizations
  that the equality saturation engine will compose together. For example, this
  term-rewriting system could simply be the operational semantics TRS of the
  programming language, in terms of PEGs (using this TRS would make the
  optimizer quite similar to a partial evaluator).
}
\item {%
  A heuristic for the runtime performance of a given PEG.
  In the most general case, this is simply a function of type
  \haskell{Term -> R} for some partially ordered semiring \haskell{R},
  which is usually an integer or floating-point type but may also be something
  more sophisticated like the symbolic integer types in the
  \href{https://hackage.haskell.org/package/sbv}{\texttt{sbv}} package.
}
\end{enumerate}

\vspace{0.5em}

The output of equality saturation is an optimized PEG, which can then be turned
back into a control flow graph for code generation. Since equality saturation
involves a time-consuming breadth-first equational proof search, there is also
an ``anytime'' variant of equality saturation where the best known PEG so far
is emitted every so often (a user-specified \textit{timer} of type
\haskell{IO Bool} is called every time a rule is applied, and if it returns
\haskell{True}, the best PEG found so far is passed to a user-specified
\textit{callback}).

Consider the types described in \autoref{lst:basic-eqsat}. This is the most
basic abstract description of equality saturation. The intended semantics of the
\haskell{saturate} function are the following:

\vspace{0.5em}

\begin{enumerate}
\item Convert the given term to a PEG, and then to an EPEG.
\item Apply a rule to a node.
\item {%
  Call the ``timer'':
  \begin{enumerate}
  \item {%
    If it returns \texttt{True}, select the best sub-PEG using the heuristic and
    call the callback with this PEG (and its quality and equality proof).
  }
  \item {%
    Otherwise, go to step 2.
  }
  \end{enumerate}
}
\end{enumerate}

\begin{listing}[ht]
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type Variable = Natural

data TermRepr = G | T

data Term (repr :: TermRepr) (node :: *) (var :: *) where
  Ref  :: Natural                      -> Term 'G   node var
  Var  :: var                          -> Term repr node var
  Node :: (node, [Term repr node var]) -> Term repr node var

type OpenTTerm   node = Term 'T node Variable
type OpenGTerm   node = Term 'G node Variable
type ClosedTTerm node = Term 'T node Void
type ClosedGTerm node = Term 'G node Void

-- Any Term can be converted to a GTerm, but not vice versa.
upcastTerm :: Term repr node var -> GTerm node var

-- Invariant: variables in right term are a subset of variables in left term
type Equation node = (TTerm node Variable, GTerm node Variable)

type TRS  node       = Set (Equation node)
type LTRS node label = InjectiveMap (Equation node) label

type TermCursor = [Natural]

data Proof label = QED | ApplyRule TermCursor label (Proof label)

type Heuristic m node real = (Term node -> m real)

type Timer m = m Bool

type Callback m node real label
  = ( Term node   -- The best term discovered so far
    , real        -- The quality of this term
    , Proof label -- A proof of equality with the original term
    ) -> m Bool   -- Returning False here will quit saturation

saturate :: (Ord real, Monad m)
         => LTRS node label            -- The rewrite rules to optimize with
         -> Heuristic m node real      -- The performance heuristic
         -> Term node                  -- The term to optimize
         -> Timer m                    -- The "timer"
         -> Callback m node real label -- The "callback"
         -> m ()
\end{minted}
\caption{The most basic exposition of equality saturation}
\label{lst:basic-eqsat}
\end{listing}

\vspace{2.5em}

Before we can discuss the laws of the \haskell{saturate} function, we must first
define some terminology:

\begin{itemize}
\item {%
  Define the
  \textit{application of an \haskell{Equation} to a \haskell{Term} focused at a \haskell{TermCursor}}
  to be the result of running the following algorithm:

  \vspace{-0.5em}
  \begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
applyFocused :: Equation n -> Term n -> TermCursor -> Maybe (Term n)
applyFocused e = go
  where
    go :: Term n -> TermCursor -> Maybe (Term n)
    go t                    [] = rewrite e t
    go (Node (n, cs)) (i : tc) = do cs' <- iforM cs (\j c -> if i == j
                                                             then go c tc
                                                             else pure c)
                                    pure Node (n, cs')
    go _                     _ = Nothing

    iforM :: (Monad m) => [a] -> (Int -> a -> m b) -> m [b]
    iforM xs f = mapM (uncurry f) (zip [0 ..] xs)
  \end{minted}

  where \haskell{rewrite :: Equation n -> Term n -> Maybe (Term n)} is the
  function that rewrites a \haskell{Term} using an \haskell{Equation} and
  returns the rewritten \haskell{Term} iff the left-hand-side of the given
  \haskell{Equation} unifies with the given \haskell{Term}.
}
\item {%
  Define the
  \textit{application of a \haskell{Proof} to a \haskell{Term} in an \haskell{LTRS}}
  to be the result of sequentially applying each rule obtained by looking up
  the preimage of the label in that \haskell{ApplyRule} node to the
  \haskell{Term}, focused at the \haskell{TermCursor} of that
  \haskell{ApplyRule} node. In code, this looks like:
  \vspace{-0.5em}
  \begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
applyProof :: LTRS n l -> Proof l -> Term n -> Maybe (Term n)
applyProof ltrs = flip go
  where
    go :: Term n -> Proof l -> Maybe (Term n)
    go t = \case
      QED -> pure t
      (ApplyRule tc l rest) -> do
        e <- equationFromLabel ltrs l
        t' <- applyFocused e t tc
        go t' rest
  \end{minted}
  where \haskell{equationFromLabel :: LTRS n l -> l -> Maybe (Equation n)}
  is the function that looks up the preimage of a label in an \haskell{LTRS}.
}
\item {%
  For most monads \texttt{m}, the denotation of a value \haskell{timer :: Timer m}
  is a function \texttt{⟦\haskell{timer}⟧} of type \haskell{Context m -> Bool},
  where \haskell{Context m} is basically a data family. In principle, execution
  of the timer function could reasonably cause side effects, but we will assume
  that these side effects cannot affect the semantics of any of the code we are
  dealing with.
  If \haskell{timer₁ :: Timer m} and \haskell{timer₂ :: Timer m}, then we say
  that $\haskell{timer₁} \subseteq \haskell{timer₂}$
  iff for every \haskell{c :: Context m},
  \texttt{⟦\haskell{timer₂}⟧ \haskell{c}} being \haskell{True}
  implies that
  \texttt{⟦\haskell{timer₁}⟧ \haskell{c}} is \haskell{True}.
}
\end{itemize}

Now we can state the informally-defined laws of the \haskell{saturate} function
as follows:

\begin{itemize}
\item {%
  Assume that the following terms exist in scope:
  \haskell{ltrs :: LTRS n l}, \haskell{h :: Heuristic m n r},
  \haskell{tᵢ :: Term n}, and \haskell{cb :: Callback m n r l}.
}
\item {%
  Suppose we call
  \texttt{\haskell{saturate} \haskell{ltrs} \haskell{h} \haskell{tᵢ} (\haskell{pure} \haskell{True}) \haskell{cb}}.

  Define \haskell{calls :: [(Term n, r, Proof l)]} to be the list containing,
  from earliest to latest, the argument of every call to \haskell{cb} at an
  arbitrary point in program execution.
}
\item {%
  For every element \haskell{(t, q, p) :: (Term n, r, Proof l)} of
  \haskell{calls}, the following properties must hold:
  \begin{enumerate}
  \item {%
    It must be the case that \haskell{t} is in the reflexive-transitive closure
    of the term-rewriting system given by \haskell{ltrs} starting
    at \haskell{tᵢ}.
  }
  \item {%
    It must be the case that applying \haskell{p} to \haskell{tᵢ}
    yields \haskell{t}.
  }
  \item {%
    It must be the case that if \texttt{cb (t, q, p) \haskell{:: m Bool}}
    evaluates to \haskell{True}, then \haskell{(t, q, p)} is the last element
    of \haskell{calls}.
  }
  \end{enumerate}
}
\item {%
  Now assume that \haskell{c₁ :: (Term n, r, Proof l)} and
  \haskell{c₂ :: (Term n, r, Proof l)} are arbitrary elements of
  \haskell{calls} such that \haskell{c₁} occurs before \haskell{c₂},
  and define \haskell{(t₁, q₁, p₁) = c₁} and \haskell{(t₂, q₂, p₂) = c₂}.
  Then $\haskell{q₁} < \haskell{q₂}$, where ${\le}$ is the partial order
  defined on the \texttt{r} type.
}
\item {%
  If \haskell{ltrs} is confluent and terminating, then the length of
  \haskell{calls} must be finite.
}
\item {%
  Suppose we have \haskell{timer₁} and \haskell{timer₂ :: Timer m} and we define
  \haskell{calls₁} and \haskell{calls₂} to be values of type
  \haskell{[(Term n, r, Proof l)]} containing, from earliest to latest,
  the argument of every call to \haskell{cb} during the execution of
  \texttt{\haskell{saturate} \haskell{ltrs} \haskell{h} \haskell{tᵢ} \haskell{timer₁} \haskell{cb}}
  and
  \texttt{\haskell{saturate} \haskell{ltrs} \haskell{h} \haskell{tᵢ} \haskell{timer₂} \haskell{cb}}
  respectively.

  Then $\haskell{timer₁} \subseteq \haskell{timer₂}$ must imply that
  \haskell{calls₁} is a subsequence of \haskell{calls₂}.
}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving performance}
\label{sec:improving-performance}

It turns out that the \naive{} implementation of the abstract interface
described in \autoref{sec:abstract-eqsat} has a variety of performance problems,
which we will address in this chapter one-by-one.

\section{Avoiding intermediate data structures}
\label{sec:avoiding-intermediates}

The algorithm described in \autoref{lst:basic-eqsat} has the \haskell{Callback}
type accepting a \haskell{Term}. Unfortunately, this means that the relevant
sub-PEG must entirely be copied every time the callback is called, and any
sharing in the PEG will be lost after this copy is done. We can solve this by
defining the \haskell{Callback} type like

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
class (Monad m) => IsTerm (m :: * -> *) (term :: * -> *) where
  getContents :: term n -> m n
  forChildren :: term n -> (term n -> m void) -> m ()

getChildren :: (IsTerm (ST s) term) => term n -> ST s [term n]
getChildren term = do
  temp <- newSTRef []
  forChildren term (\child -> modifySTRef' (child :) temp)
  readSTRef temp

instance (Monad m) => IsTerm m ClosedTTerm where
  getContents (Node (n,        _)) = pure n
  forChildren (Node (_, children)) = mapM_ f children

data Callback (m :: *) (node :: *) (real :: *) (label :: *) :: * where
  Callback :: (IsTerm m term)
           => ((term node, real, Proof label) -> m Bool)
           -> Callback m node real label
\end{minted}

so that it wraps up an \haskell{IsTerm} dictionary, rather than requiring a
concrete \haskell{Term} value. We can then define an instance
of \haskell{IsTerm} on a (rooted) PEG type, so that \haskell{saturate} can
instantiate the existentially quantified type variable bound by the
\haskell{Callback} constructor to that type.

The reason \haskell{IsTerm} has an extra type parameter corresponding to the
ambient monad is to account for the possibility that the type we are using
for PEGs requires a constraint on the ambient monad (e.g.: since a PEG is
mutable it will likely want a \texttt{m \textasciitilde{} \haskell{ST} s} or
\texttt{m \textasciitilde{} \haskell{IO}} or \haskell{PrimMonad m} constraint,
where \haskell{PrimMonad} is a typeclass defined in the
\href{https://hackage.haskell.org/package/primitive-0.6.3.0/docs/Control-Monad-Primitive.html#t:PrimMonad}{\texttt{primitive}}
package encompassing monads like \haskell{ST s} and \haskell{IO}
and \haskell{ReaderT Foo IO} that allow mutation).

\section{Automatically-applied rules}
\label{sec:automatic-rules}

In some cases, we may have knowledge that a rule is \textit{always} an
improvement, regardless of the assignment of terms to its metavariables.
For example, if our cost heuristic is expressed symbolically as a function
that can be sent to an SMT solver, then for each rule we may try using the
SMT solver to prove that there is an assignment that makes the cost heuristic
worse after applying the rule. Then if the SMT solver returns \texttt{unsat},
we know that the rule is always an improvement.

If we have this knowledge, then it makes sense to make these rules
\textit{automatic}, in the sense that they are always applied whenever possible,
before any other rules. Of course, this makes our optimization algorithm greedy,
since the application of these automatic rules may prevent other rules from
being applied (and these other rules may be better optimizations). It may be
possible to avoid this greediness by also accepting a proof that the rule
doesn't affect the set of applicable optimizations, but I suspect that this
constraint will be so strong that it excludes nearly all of the relevant rules.
Finding the least-restrictive constraint on rules for automatic application is
an interesting subject for future research.

\section{Maximal sharing}
\label{sec:msharing}

The version of equality saturation I have shown you thus far has a major issue:
it doesn't account for sharing in the EPEG. We can solve this via hash-consing,
although the presence of cycles in the EPEG presents a problem for most
hash-consing algorithms. The solution is to compute the set of
strongly-connected components (SCCs) of the EPEG, and then hash-cons the graph
the nodes of which are pairs of EPEG nodes and the SCCs they belong to and
the edges of which are a spanning tree of the EPEG. Pairing the nodes with their
SCCs ensures that the cycles are included in the hash of each node, and an EPEG
can be recovered by reintroducing the cycles from the SCCs.

\section{Cycle detection}
\label{sec:cycle-detection}

Consider any term rewriting system with a rule like $f(x, y) \mapsto f(y, x)$
for some function symbol $f$; it is clear that such a rewrite rule will cause
a \naive{} implementation of equality saturation to run forever on any term
including $f$, since the rule can always be applied. In many cases we can
prevent such loops in the following way:

\begin{itemize}
\item {%
  If there are $n$ rules in the term rewriting system, associate a bitvector of
  length $n$, filled with zeros, to each node in the EPEG. For a node $A$, we
  will denote this bitvector by $v_A \in \{0, \ldots, n - 1\} \to \mbs{2}$.
}
\item {%
  When we merge nodes $A$ and $B$ during hash-consing into a node $C$, it should
  be the case that
  $ v_C
  = {\lor}_\msf{bitwise}(v_A, v_B)
  = \{(i, v_A(i) \lor v_B(i)) \mid i \in \{0, \ldots, n - 1\}\}$.
}
\item {%
  If the $i$th rule in the term rewriting system is going to be applied to
  node $A$, first check whether $v_A(i) = 1$. If so, then skip applying this
  rule. Otherwise, set the value of $v_A(i)$ to $1$ and apply the rule.
}
\end{itemize}

Unfortunately, as the number of rules increases, this solution becomes slower
and slower, as the bitvectors will tend to be sparse (mostly zeros). Ideally,
an adaptive data structure would be used that is the same as a bitvector as
long as the size of the bitvector is smaller than that of a cache line, and once
this limit is reached, it becomes a trie or hashset. Since the number of rules
is completely known as soon as equality saturation begins, such an adaptive data
structure need not have any runtime overhead relative to its underlying data
structures.

\section{Online SCC computation}
\label{sec:online-scc}

The sharing algorithm described in \autoref{sec:msharing} requires recomputation
of the strongly-connected component graph of the EPEG every time a node is
added. This is inefficient, since most of the work done in each SCC computation
will be similar before and after the addition of a node. This inefficiency can
be prevented by using any of the online algorithms for SCC computation. As of
this writing, the state of the art for online SCC computation is described in
a 2015 paper~\cite{bender-2015} by Bender et al. There are two algorithms
defined in the paper by Bender: one that is optimal for sparse graphs, and one
that is optimal for dense graphs. It is not clear to me which algorithm is more
relevant to equality saturation, since although it seems at first glance that
our graphs are sparse, the whole point of an EPEG is to maximize sharing, so
it may be that the graphs end up being dense. This will probably require
empirical study; it is likely that constant factors are a bigger concern than
the asymptotics of these two algorithms.

\section{Term indexing}
\label{sec:term-indexing}

\textit{Term indexing} is a field of study in which the problem of compactly
storing a large number of open terms such that the index can be queried for a
subset that includes the set of added terms that will unify with a query term
\cite{handbook-ch26}. A term index is best described by the typeclass shown in
\autoref{lst:term-index}.

A term index effectively acts as a filter that can be applied before matching;
it allows false positives, but does not allow false negatives, much like a
Bloom filter~\cite{bloom-1970}. By filtering the set of patterns before we do
pattern matching, we can decrease the amount of work that needs to be done
during pattern matching. A term index is a \textit{perfect filter} if it never
returns a false positive; otherwise it is an \textit{imperfect filter}.

\begin{listing}[b]
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type Key node var = (Ord node, Ord var)
class TermIndex (index :: * -> * -> * -> *) where
  -- An injective type family defining the mutable version of an immutable
  -- term index. The extra s parameter is for an ST-style state token.
  type Mut index (node :: *) (var :: *) (value :: *) (s :: *)
    = (result :: *) | result -> index

  empty        :: index node var value
  freeze       :: Mut index node var value s
               -> ST s (index node var value)
  thaw         :: index node var value
               -> ST s (Mut index node var value s)
  insertMany   :: (Key node var)
               => Mut index node var value s
               -> [(TTerm node var, value)]
               -> ST s ()
  queryMany    :: (Key node var, Monad m)
               => index node var value
               -> [(TTerm node var, value -> m any)]
               -> m ()
  queryManyMut :: (Key node var)
               => Mut index node var value s
               -> [(TTerm node var, value -> ST s any)]
               -> ST s ()
\end{minted}
\caption{A typeclass describing a term index}
\label{lst:term-index}
\end{listing}

Every \haskell{TermIndex} has a mutable (\haskell{Mut}) version and an immutable
version. The \haskell{freeze} and \haskell{thaw} methods allow interconversion
between these types. The \haskell{empty} method is the only way to create a term
index from nothing. The laws of the \haskell{TermIndex} typeclass are:

\vspace{-1em}
\begin{align}
  \mtt{\haskell{thaw} \haskell{>=>} \haskell{freeze}}
  & = \haskell{pure}
  \label{eq:term-index-law-1} \\
  \mtt{\haskell{insertMany} $i$ \haskell{[]}}
  & = \haskell{pure ()}
  \label{eq:term-index-law-2} \\
  \mtt{\haskell{insertMany} $i$ ($x \mathbin{\haskell{++}} y$)}
  & = (\mtt{\haskell{insertMany} $i$ $x$ \haskell{>>} \haskell{insertMany} $i$ $y$})
  \label{eq:term-index-law-3} \\
  \mtt{\haskell{queryMany} $i$ \haskell{[]}}
  & = \haskell{pure ()}
  \label{eq:term-index-law-4} \\
  \mtt{\haskell{queryMany} $i$ $(x \mathbin{\haskell{++}} y)$}
  & = (\mtt{\haskell{queryMany} $i$ $x$ \haskell{>>} \haskell{queryMany} $i$ $y$})
  \label{eq:term-index-law-5} \\
  \mtt{\haskell{queryMany} \haskell{empty} $q$}
  & = \haskell{pure ()}
  \label{eq:term-index-law-6} \\
  \mtt{\haskell{queryManyMut} $i_m$ $q$}
  & = (\mtt{\haskell{freeze} $i_m$}
    \:\: \haskell{>>=} \:\:
    (\lambda i \, \mapsto \, \mtt{\haskell{queryMany} $i$ $q$}))
  \label{eq:term-index-law-7}
\end{align}

The reason I define the fundamental insertion and querying operations of a
\haskell{TermIndex} as \haskell{insertMany} and \haskell{queryMany}, rather
than as

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
class TermIndex (index :: * -> * -> * -> *) where
  -- ... other definitions ...
  insert :: (Key n v) => Mut index n v val s -> TTerm n v -> val -> ST s ()
  query  :: (Key n v, Monad m) => index n v val -> TTerm n v -> m [val]
\end{minted}
\vspace{0em}

is for two reasons. Firstly, there are some term indexing data structures that
are more efficient when you are inserting or querying multiple terms at the same
time, and since \haskell{insert} and \haskell{query} can be defined in terms of
\haskell{insertMany} and \haskell{queryMany} relatively easily, it makes more
sense for \haskell{insertMany} and \haskell{queryMany} to be the primitive
operations. Secondly, by having \haskell{queryMany} accept a monadic callback
rather than returning a list of results, we can avoid the creation of
intermediate data structures.

While it is possible to define \haskell{insert} and \haskell{query} in terms of
\haskell{insertMany} and \haskell{queryMany} respectively, it causes an
additional allocation and pointer indirection for the singleton list, so in an
actual version of the \haskell{TermIndex} typeclass, \haskell{insert} and
\haskell{query} would be defined as methods with default implementations.
For additional ease of defining instances, \haskell{insertMany} and
\haskell{queryMany} can also have default implementations in terms of
\haskell{insert} and \haskell{query} respectively, and to avoid people writing
empty instances (which would loop infinitely), a \texttt{MINIMAL} pragma can be
added, which will allow GHC to check that a valid set of methods have been
defined.

\subsection{Discrimination trees}
\label{sec:discrimination-trees}

A \textit{discrimination tree} is a term-indexing data structure based on a
trie~\cite{handbook-ch26}. Define the \textit{path} of a term
\haskell{t :: TTerm n v} to be a value of type \haskell{[Maybe n]} representing
the preorder traversal of the term, replacing any variable nodes with
\haskell{Nothing}. Inserting a term-value-pair into a discrimination tree is the
same as inserting the path computed from the term along with the value into a
trie of type \haskell{Trie (Maybe n) val}. Querying a discrimination tree for a
given term \haskell{q :: TTerm n v} is the same as computing the path of
\haskell{q} and doing a backtracking search through the trie for matching paths;
this is much the same as the traditional trie retrieval algorithm, except that
when the \haskell{Maybe n} value you're searching for at a node is
\haskell{Nothing}, you nondeterministically search in all the the children, and
when the value you're searching for at a node is \haskell{Just n}, you
nondeterministically search in the \haskell{n} and \haskell{Nothing} children.

Note that discrimination trees only work when your terms don't include variadic
function symbols. If they do include variadic function symbols, you can still
use discrimination trees by simply augmenting your node type with the number of
children at that node, effectively introducing one function symbol for every
possible variadic usage.

For cache efficiency, it is ideal to implement a discrimination tree using a
trie based on a contiguous growable array (and the children of each node should
be referenced with \haskell{Word32} offsets rather than pointers, since in
practice the tree will rarely exceed $\SI{4}{\gibi\byte}$).

\subsection{Substitution trees}
\label{sec:substitution-trees}

A \textit{substitution tree} is a term-indexing data structure based on a tree
of substitutions with values at leaf nodes~\cite{graf-1994,handbook-ch26}.
The intended semantics of such a tree is that each substitution in composed
with the substitutions above it to compute a filter for terms.

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type Sub node var = Map var (TTerm node var)

domain :: Sub node var -> Set var

type Indicator = Int

data SubTree node var value
  = Leaf value
  | Branch (Sub node (Either Indicator var)) [SubTree node var value]
\end{minted}
\vspace{0em}

There are three invariants that must be true for the \haskell{SubTree} type,
which may be enforced by making \haskell{SubTree} an abstract type in its own
module. Firstly, if the size of the list of children at a \haskell{Branch} node
is $n \in \mbb{N}$, then it must always be true that $n \neq 1$. Secondly, for
every path
\texttt{[(\haskell{σ₁}, \haskell{ω₁}), …, (\haskell{σₙ}, \haskell{ωₙ})] \haskell{::} \haskell{[Sub n (Either Indicator v), [SubTree n v val]]}}
from the root to a leaf of a non-empty tree it must be the case that the
variables introduced by the composition $\haskell{ωₙ} \comp \ldots \comp \haskell{ω₁}$ are
all \texttt{\haskell{Left} \haskell{i}} for some \haskell{i :: Indicator}.
Thirdly, for every such path and every $j \in \{1, \ldots, n - 1\} \subset \mbb{N}$,
it must be the case that, when we define $k = j + 1$,
\texttt{\haskell{null} (\haskell{domain} \haskell{σₖ} ∩ (\haskell{domain} \haskell{σ₀} ∪ … ∪ \haskell{domain} \haskell{σⱼ})) \haskell{==} \haskell{True}}.

Refer to \cite{graf-1994} for a description of the insertion and querying
algorithms on substitution trees.

% Querying a substitution tree for a term \haskell{q :: TTerm n v} involves doing
% a backtracking search; \sfixme{}.

\section{Caching performance heuristic executions}
\label{sec:heuristic-caching}

One of the disadvantages of the version of equality saturation described
thus far is that the heuristic must be run on every node of the EPEG
every time the timer returns \haskell{True}. For a large class of heuristics,
it turns out that this is not actually necessary. Since the EPEG is
referentially transparent, any heuristic that is definable solely in terms of
the semantics of a node can be expressed as a catamorphism of type
\haskell{(node, [real]) -> real}. A sufficient but not necessary condition for
a heuristic being definable in this way is if the definition of semantics is
purely operational in nature; this is true in most strictly evaluated languages
but not in many lazy languages.

If a heuristic is definable as a catamorphism, then we can cache heuristic
executions by noticing that any subtree of the EPEG that is unchanged between
the current EPEG and the last time the best sub-PEG was selected will continue
to have the same best sub-PEG.

We can take this further, however. If we know that we have saturated a subtree
with equalities (e.g.: since the cycle-detection algorithm described in
\autoref{sec:cycle-detection} collects information on what rules have been
applied to a given node, we can propagate a token that represents the fact that
a given subtree is saturated), then we can entirely throw out (``garbage
collect'') all the other (non-optimal) sub-PEGs in that subtree. Note that when
we throw away the non-optimal sub-PEGs, we don't reset the cycle-detection
data structures associated with each node, thereby preventing the saturation
algorithm from exploring already-explored parts of the search space.

There is a slight issue with this ``garbage collection'' strategy. Suppose that
we garbage collect some nodes, and then another part of the EPEG ends up being
rewritten to a term that would have gotten shared with the garbage collected
nodes; this will cause the saturation algorithm to repeat work it has already
done! To prevent this, we can ``axiomatize'' the optimizations: for each element
of the equivalence class of terms generated by the sub-EPEG we are ``garbage
collecting'', we will introduce a (derivable) rewrite rule with ground terms on
both sides to the term-rewriting system used for equality saturation.
This added rewrite rule will have to be specially marked so that the
cycle-detection data structure gets correctly initialized after applying
it; since we know that the result of that rewrite rule is already the best in
its equivalence class, we don't want to allow rewriting out of the result
(e.g.: if a bitvector is used for cycle-detection, it should be filled with $1$s
rather than the usual $0$s). Of course, even with the space savings given
by our term index, these added axioms will take up at least as much space as the
nodes we just garbage collected; the goal here is actually to eliminate
duplication of edges relating to proofs of equality and wasted time during
combinatorial optimization. This garbage collection idea also prevents
equality saturation from being used for translation validation, so it is perhaps
not worth implementing.

It is unclear to me whether any of these ideas will actually create speed
improvements in practice, but they are probably worth exploring.

\section{Parallelization}
\label{sec:parallelization}

Since equality saturation is, at its heart, a breadth-first search, it seems
quite amenable to parallelization. However, the maximal sharing algorithm
described in \autoref{sec:msharing} creates a problem; worker threads cannot
work independently on different parts of the graph without mutexes if sharing
might combine these independent sections. There are several possible solutions
to this, but the simplest to implement is that the maximal sharing algorithm
should run independently of the worker threads, locking the entire graph while
deduplication occurs. In this exposition of equality saturation, we need to
freeze the entire graph anyway to capture the corresponding \haskell{TTerm} to
be passed to the callback, so this seems like not a huge loss, but there may be
a way to implement parallel anytime equality saturation with less time devoted
to blocking.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving expressiveness}
\label{sec:improving-expressiveness}

In \autoref{sec:improving-performance} we discussed ways to improve the
performance of equality saturation. In this chapter, we will discuss ways to
improve the \textit{expressiveness} of equality saturation, in two senses:
some of these improvements will make the set of languages optimizable with a
generic equality saturation implementation larger, while the rest will simply
make it easier to use the equality saturation implementation.

\section{Variadic function symbols}
\label{sec:variadic-function-symbols}

Many languages contain constructors that are in some sense variadic.
This variadicity can be eliminated by adding a constructor to the \haskell{Term}
type that allows for variadic function symbols. This would allow term indexing
data structures like discrimination trees, which do not ordinarily handle
variadic function symbols, to be expressed in a more type-safe way.

\section{Adding equational axioms}
\label{sec:equational-axioms}

Many languages contain constructors that should be considered equal under some
non-syntactic equivalence relation. For example, if you have multiple
independent modules in the code you are optimizing, then cross-module inlining
is only possible if the constructor for a set of modules is actually considered
modulo associativity and commutativity. You can, of course, add rewrite rules
to your theory corresponding to associativity and commutativity, but in many
cases this will be much slower (due to spurious copying) than a dedicated solver
for associative-commutative matching. There are a number of theories that can
be integrated into the matching algorithm, including:

\begin{itemize}
\item Semigroups (\textsc{a})
\item Abelian semigroups (\textsc{ac})
\item Abelian monoids (\textsc{acu})
\item Idempotent abelian semigroups (\textsc{aci})
\item Boolean rings (\textsc{br})
\item Abelian groups (\textsc{ag})
\end{itemize}

For a relatively comprehensive review of the literature on equational matching
and unification, see \cite{siekmann-1989}. Further research is needed on the
asymptotic and in-practice complexity of these matching algorithms to determine
which ones are relevant to a practical implementation of equality saturation,
but given that the $\mbb{K}$ Framework implements \textsc{ac}-unification, it is
likely that this at least is worth implementing.

\section{Conditional rewriting}
\label{sec:conditional-rewriting}

Many optimizations cannot easily be implemented using a term-rewriting system
alone; it is often much easier to write optimizations in a \textit{conditional}
term-rewriting system, in which rules can have boolean side conditions.

We can improve the expressiveness of our equality saturation algorithm by
implementing conditional rewriting. One efficient way to implement this is by
eliminating a conditional TRS into an equivalent unconditional TRS using the
algorithm described in \cite{rosu-2005}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving correctness}
\label{sec:improving-correctness}

In this chapter we will discuss ways to decrease the chance of user error when
using an equality saturation implementation.

\section{Confluence-checking}
\label{sec:confluence-checking}

Although the term-rewriting systems we accept for equality saturation may not be
terminating, it seems likely that they will be confluent. Thus it may be
desirable to enforce this condition in the type of \haskell{saturate}; it should
only accept a confluent term-rewriting system.

How, then, do we define the type of confluent term-rewriting systems? Simple:
a pair of a term-rewriting system and a proof certificate of its confluence,
in some easy-to-check logic like ZF set theory or type theory. The type of
confluent term-rewriting systems will then be an abstract type whose constructor
will fail to return if the given proof certificate fails to check (or if the
proof certificate is not actually a proof of confluence for the associated TRS).
Since we only care about the ability to check a proof certificate and the
ability to determine if a given proof certificate is a proof of confluence, we
can even be polymorphic in the logic used by defining an appropriate typeclass.
The user may want an escape hatch for non-confluent or
confluent-but-difficult-to-prove-confluent TRSes, which should be integrated
into the logic as a proof of falsehood.

Often, the user will not want to manually prove confluence for their TRS.
As it happens, there are a number of pre-existing algorithms for conservatively
checking whether a given term-rewriting system is confluent; see
\cite{felgenhauer-2015} for more details. I am not sure if these algorithms
emit proof certificates; if they do not, the escape hatch can be used
(though obviously in this case it is worth writing extensive tests for the
confluence checker to avoid bugs).

\section{Type-checking}
\label{sec:type-checking}

%To avoid confusion, in this section we will refer to the types in the language
%the user gives us (which are value-level entities in the equality saturation
%implementation) as \textit{sorts}.

If the user gives us a partial function that assigns sorts to terms with
metavariables (which should be the case for any language with lambda syntax
and mandatory type annotations) and a subsorting partial order, then we can
check the validity of the \haskell{Equation}s that make up the \haskell{LTRS}
given to us by the user.

We do this for each \haskell{Equation} by first using the partial function to
compute the inferred sorts of the LHS and its metavariables. Then we compute the
inferred sorts of the RHS and its metavariables. For each metavariable, we
ensure that the sort inferred for the use site in the RHS is a subsort of the
sort inferred for the binding site in the LHS. Finally, we ensure that the sort
of the RHS is a subsort of the sort of the LHS and vice versa (the subsort
relation is reflexive, so this means the two sorts are equivalent).
%It is not quite clear to me if this last requirement is maximally loose; it may
%prevent some otherwise-valid optimizations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \chapter{Integration with other tools}
% \label{sec:integration}
%
% \fixme{introduction paragraph}
%
% \section{The $\mathbb{K}$ Framework}
% \label{sec:k-framework}
%
% Citation: \cite{rosu-2010}
%
% \sfixme{}
%
% \section{$\mathrm{IncA}$ / $\mathrm{IncA}_L$}
% \label{sec:inca-incal}
%
% Citation: \cite{szabo-2016}, \cite{szabo-2017}
%
% \sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Example IR formats}
\label{sec:example-languages}

\section{Cartesian closed categories}
\label{sec:cccs}

It is well-known that the internal logic of Cartesian closed categories is
the simply-typed lambda calculus. Conal Elliott describes in a 2017 paper
\cite{elliott-2017} a method of converting terms in the simply-typed lambda
calculus to algebraic expressions in the signature given by

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
class CCC ((~>) :: * -> * -> *) where
  -- The internal hom of the category
  type (|=>) (~>) :: * -> * -> *
  id      :: a ~> a
  (.)     :: (b ~> c) -> (a ~> b) -> (a ~> c)
  fork    :: (a ~> x) -> (a ~> y) -> (a ~> (x, y))
  exl     :: (a, b) ~> a
  exr     :: (a, b) ~> b
  apply   :: ((a |=> b), a) ~> b
  curry   :: ((a, b) ~> c) -> (a ~> (b |=> c))
  uncurry :: (a ~> (b |=> c)) -> ((a, b) ~> c)
\end{minted}

Since this representation of the simply-typed lambda calculus has no binders, it
can be used with equality saturation, though care has to be taken with the
performance heuristic if the code is to be evaluated lazily.

Similar tricks may be possible with other logics internal to categories that
are expressible in Haskell, see the chart relating flavors of type theory to
flavors of category theory in
\cite{nlab-relation-between-type-theory-and-category-theory}.

% \section{Interaction nets}
% \label{sec:interaction-nets}
%
% Citation: \cite{asperti-1998}
%
% \sfixme{}

\section{Term-rewriting systems}
\label{sec:term-rewriting-systems}

In unpublished correspondence between Adithya Murali, Alexander Altman, and
Remy Goldschmidt (myself), we came up with a method for expressing a
term-rewriting system \textit{itself} as an entity that can be optimized using
equality saturation.

The datatype equivalent to the type of terms for this system is given as
\haskell{TermRewritingSystem}:

\vspace{-0.5em}
\begin{minted}[frame=lines,framesep=2mm,linenos]{haskell}
type FunctionSymbol = Int
type Label = Text

data Rule = Rule [Label] (OpenTTerm FunctionSymbol) (OpenTTerm FunctionSymbol)

data TermRewritingSystem = TermRewritingSystem (Set Rule)
\end{minted}

Note that because \haskell{TermRewritingSystem} uses the \haskell{Set} type in
its definition, we need to use \textsc{ac}-matching when implementing this.

The primitive rules used in equality saturation for this language are
composition of rules to create new rules (with the lists of labels concatenated)
and deletion of an unlabelled rule $r = X \mapsto Y$ if the commutative diagram
below is satisfied:

\begin{figure}[h!]
\centering
\begin{tikzcd}
I
\arrow[r,hook]
\arrow[r,dashed,to=Y,bend left=50]
& X \arrow[r,"r"]
& |[alias=Y]| Y
\end{tikzcd}
\label{fig:deletion-cd}
\end{figure}

This is a necessary condition for a rule to be deleted; in words, we would
describe it as ``for every instance $I$ of the pattern term $X$, there must
exist another way to get to $Y$''.

\section{Relational algebra}
\label{sec:relational-algebra}

Relational algebra, which is a mathematical formalization of a fragment of the
languages (like SQL) commonly used for querying relational databases, seems like
an ideal language to optimize with equality saturation. It has many equational
laws and is already completely referentially transparent.

The same is not true of Datalog, a non-Turing complete fragment of Prolog that
is more expressive than relational algebra, but there may be a way to express
Datalog queries in a referentially transparent syntax; I suspect it may look
like augmenting relational algebra with some kind of fixed-point operator, but
I am not sure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Future Work}
\label{sec:future-work}

\section{Incremental equality saturation}
\label{sec:incremental-eqsat}

If there is an algorithm described by a function \haskell{f :: A -> B}, we say
that \haskell{f' :: (A, ΔA) -> B} is the \textit{incrementalized} version of
\haskell{f} if \haskell{ΔA} is a monoid and there exists a monoid action
\haskell{patch :: ΔA -> A -> A} such that for any \haskell{x :: A} and
\haskell{δ :: ΔA},
\texttt{\haskell{f'} (\haskell{x}, \haskell{δ}) = \haskell{f} (\haskell{patch} \haskell{δ} \haskell{x})}.
Ideally, if \haskell{x} is already computed and \haskell{δ} is ``small'', the
incrementalized version \texttt{\haskell{f'} (\haskell{x}, \haskell{δ})} will
also be faster to compute than
\texttt{\haskell{f} (\haskell{patch} \haskell{δ} \haskell{x})},
though this connotation is hard to formalize, especially in a
lazy language like Haskell.

We can likely usefully incrementalize equality saturation in this sense, as
referential transparency ensures that we only need to re-optimize the parts of
the PEG that have changed.

This would probably look something like:

\begin{enumerate}
\item {%
  Compute the difference of the old and new PEGs using
  a modified version of \cite{lempsink-2009}.
}
\item {%
  If there are common subtrees, apply the proofs from the old EPEG
  to the new EPEG.
}
\item {%
  Exclude the already-explored proof tree branches in the new EPEG.
}
\end{enumerate}

Alternatively, we may simply be able to reuse the pre-existing EPEG by adding
the nodes of the new PEG to optimize to the EPEG, running the maximal sharing
algorithm on it, and removing all nodes that are not in the connected component
containing the root node of that PEG.

This feature seems especially important for real-world compilers using equality
saturation, as it could drastically reduce build times when optimization is
enabled and small changes have been made. There are plenty of unexplored
questions in this realm; for example, how should the EPEG be quickly and
compactly serialized to avoid overhead due to this feature?

\section{Dependent types}
\label{sec:dependent-types}

Code written in languages with dependent type systems often ends up containing
many theorems and equations encoded in higher-order logic at the type level.
These equations could be useful for equality saturation, as they represent
(often nontrivial) ways of rewriting a program. Selsam and Moura wrote a 2016
paper \cite{selsam-2017} on congruence closure in intensional type theory that
seems especially relevant to an implementation of equality saturation for a
dependently-typed language.

\section{Homotopy type theory}
\label{sec:hott}

Homotopy type theory is a constructive foundation for mathematics based on
intensional type theory with higher inductive types and the univalence axiom,
which is incompatible with the \textit{uniqueness of identity proofs} axiom
commonly assumed in dependent type theory.

The main thing distinguishing proofs in homotopy type theory from proofs in
dependently typed languages is the fact that there can be nontrivial equalities
between type-level equalities. In fact, types in homotopy type theory can
usefully be thought of as weak $\omega$-groupoids, which can have arbitrarily
tall towers of nontrivial equality. Cubical type theory is an experimental
computable implementation of homotopy type theory, so homotopy type theory may
have relevance to computer applications.

It would be interesting to explore saturation-based automated theorem proving
in the context of these weak $\omega$-groupoids, rather than the $1$-groupoids
(equivalence relations) we've considered in this paper.

% \section{Laziness}
% \label{sec:laziness}
%
% Citation: \cite{okasaki-1998}
%
% \sfixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{easylist}[itemize]
% @ Equality saturation
% @@ \cite{tate-2009}
% @@ \cite{tate-2012-eqsat}
% @@ \cite{stepp-2011}
% @ Proof generalization
% @@ \cite{tate-2012-proofgen}
% @ Term indexing
% @@ \cite{handbook-ch26}
% @@ \cite{graf-1994}
% @@ \cite{holen-2013}
% @ Unification / Matching
% @@ \cite{baader-2015}
% @@ \cite{plotkin-1972}
% @@ \cite{eker-2003}
% @@ \cite{eker-1995}
% @@ \cite{forgy-1979}
% @@ \cite{doorenbos-2001}
% @ Resource Estimation
% @@ \cite{carbonneaux-2015}
% @@ \cite{hoffmann-2012}
% @ Static Analysis
% @@ \cite{calcagno-2011}
% @@ \cite{brotherston-2017}
% @ SMT solving
% @@ \cite{bjorner-2015}
% @ Reference
% @@ \cite{baader-1998}
% @ Miscellaneous
% @@ \cite{alpuente-2014}
% @@ \cite{lewenstein-2013}
% @@ \cite{stampoulis-2010}
% @@ \cite{sjoberg-2014}
% @@ \cite{bachmair-2003}
% @@ \cite{payet-2008}
% @@ \cite{baumgartner-2007}
% @@ \cite{darais-2017}
% \end{easylist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[heading=bibnumbered]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
